{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"/kaggle/input/lmsys-chatbot-arena\")\n",
    "\n",
    "TARGETS = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_PATH / \"train.csv\")\n",
    "test = pd.read_csv(DATA_PATH / \"test.csv\")\n",
    "sub = pd.read_csv(DATA_PATH / \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_appearance\"></a>\n",
    "## 1. Data Appearance\n",
    "[**<span style=\"color:#FEF1FE; background-color:#535d70;border-radius: 5px; padding: 2px\">Go to Table of Content</span>**](#toc)\n",
    "\n",
    "After data is loaded, let's see what information is provided in training and test sets. Following is the file description of `train.csv`,\n",
    "* `id` - A unique identifier for the row.\n",
    "* `model_[a/b]` - The identity of model_[a/b]. Included in `train.csv` but not `test.csv`.\n",
    "* `prompt` - The prompt that was given as an input (to both models).\n",
    "* `response_[a/b]` - The response from model_[a/b] to the given prompt.\n",
    "* `winner_model_[a/b/tie]` - Binary columns marking the judge's selection. The ground truth target column.\n",
    "\n",
    "<a id=\"quick_sum\"></a>\n",
    "### *Quick Summary*\n",
    "[**<span style=\"color:#FEF1FE; background-color:#535d70;border-radius: 5px; padding: 2px\">Go to Table of Content</span>**](#toc)\n",
    "1. There are 57477 training rows and 3 test rows.\n",
    "    * **Note:** Test data will be replaced with **the full test set (~25k rows, 70% for private LB)** during scoring phase.\n",
    "2. The column `id` has no duplicated values.\n",
    "3. Model identities aren't revealed in the test set.\n",
    "    * **Question:** Can we try to predict model identities first and then use this auxiliary information for predicting preferences?\n",
    "4. Strings in columns `prompt`, `response_a`, and `response_a` are wrapped in a list. The reason is that each chat can **contains more than one prompt/response pairs**.\n",
    "5. There exist 14 duplicated rows (7 groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape | train (57477, 9) | test (3, 4)\n",
      "--------------------------------------------------\n",
      ">>> The First 3 Training Rows <<<\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             model_a         model_b  \\\n",
       "0  30192  gpt-4-1106-preview      gpt-4-0613   \n",
       "1  53567           koala-13b      gpt-4-0613   \n",
       "2  65089  gpt-3.5-turbo-0613  mistral-medium   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      ">>> The First 3 Test Rows <<<\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[\"I have three oranges today, I ate an orange ...</td>\n",
       "      <td>[\"You have two oranges today.\"]</td>\n",
       "      <td>[\"You still have three oranges. Eating an oran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[\"You are a mediator in a heated political deb...</td>\n",
       "      <td>[\"Thank you for sharing the details of the sit...</td>\n",
       "      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>[\"How to initialize the classification head wh...</td>\n",
       "      <td>[\"When you want to initialize the classificati...</td>\n",
       "      <td>[\"To initialize the classification head when p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  [\"I have three oranges today, I ate an orange ...   \n",
       "1   211333  [\"You are a mediator in a heated political deb...   \n",
       "2  1233961  [\"How to initialize the classification head wh...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                    [\"You have two oranges today.\"]   \n",
       "1  [\"Thank you for sharing the details of the sit...   \n",
       "2  [\"When you want to initialize the classificati...   \n",
       "\n",
       "                                          response_b  \n",
       "0  [\"You still have three oranges. Eating an oran...  \n",
       "1  [\"Mr Reddy and Ms Blue both have valid points ...  \n",
       "2  [\"To initialize the classification head when p...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Data Shape | train {train.shape} | test {test.shape}\")\n",
    "print(f\"-\" * 50)\n",
    "print(f\">>> The First 3 Training Rows <<<\")\n",
    "display(train.head(3))\n",
    "print(f\"-\" * 50)\n",
    "print(f\">>> The First 3 Test Rows <<<\")\n",
    "display(test.head(3))\n",
    "assert train[\"id\"].nunique() == len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to drop `id` column directly because it has nothing to do with our analysis task. But, we leave `id` in `test` untouched for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping 'id' column, shape of the training DataFrame becomes (57477, 8).\n",
      "--------------------------------------------------\n",
      ">>> The First 3 Training Rows <<<\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_a         model_b  \\\n",
       "0  gpt-4-1106-preview      gpt-4-0613   \n",
       "1           koala-13b      gpt-4-0613   \n",
       "2  gpt-3.5-turbo-0613  mistral-medium   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop(\"id\", axis=1)\n",
    "print(f\"After dropping 'id' column, shape of the training DataFrame becomes {train.shape}.\")\n",
    "print(f\"-\" * 50)\n",
    "print(f\">>> The First 3 Training Rows <<<\")\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exist 14 duplicated rows forming 7 groups, we just keep one row per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There exist 14 duplicated rows.\n",
      "After removing duplicates, #samples drops from 57477 to 57470.\n"
     ]
    }
   ],
   "source": [
    "n_dups = train.duplicated(keep=False).sum()\n",
    "print(f\"There exist {n_dups} duplicated rows.\")\n",
    "\n",
    "n_rows1 = len(train)\n",
    "train = train.drop_duplicates(keep=\"first\", ignore_index=True)\n",
    "n_rows2 = len(train)\n",
    "print(f\"After removing duplicates, #samples drops from {n_rows1} to {n_rows2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"one_chat\"></a>\n",
    "### *One Chat*\n",
    "\n",
    "Then, let's take the first row (*i.e.,* the first chat) as an example to see what information is contained in one sample. Through displaying the raw text, we observe,\n",
    "1. The prompt and both reponses are of **string** type and also **wrapped in a list**.\n",
    "    * One chat can **contain more than one prompt/response pairs**. \n",
    "2. There exist escape characters (*e.g.,* a new line `\\n`).\n",
    "3. Markdown syntax is used (*e.g.,* double asterisks `****` for strong words).\n",
    "4. There exist ordered lists.\n",
    "    * Unordered lists are also found in othere examples.\n",
    "5. There exist unicode characters (*e.g.,* emoji `\\ud83c\\udf4d`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt ===\n",
      "[\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"]\n",
      "\n",
      "=== Response of gpt-4-1106-preview ===\n",
      "[\"The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\n\\nHere are some arguments in favor of and against such policies:\\n\\n**Arguments in favor:**\\n\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\n\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\n\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\n\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\n\\n**Arguments against:**\\n\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\n\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\n\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \\\"tokens,\\\" undermining their legitimacy and potentially leading to resentment among colleagues.\\n\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\n\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one's ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \\\"color-blind\\\" or \\\"gender-blind\\\" approach to hiring and promotions.\\n\\nOrganizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions.\",\"Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battle and pizza purists protest.\\n\\nLet's slice into the debate with a zest of fun:\\n\\n**Team Pineapple:** \\\"Absolutely, yes! Pineapple on pizza is like a beach party in your mouth. The sweet juiciness of pineapple chunks frolicking with savory ham or bacon creates a flavor wave that surfs across the cheesy ocean of deliciousness. It's the Mardi Gras of pizzas, where sweet meets savory in a jubilant jamboree!\\\"\\n\\n**Team No-Pineapple:** \\\"No way, not in a million pizzas! Pineapple is a fruit that should be sunbathing on a tropical fruit platter, not freeloading on a sacred slice of pizza. The thought of warm, melty cheese conspiring with pineapple's sugary mischief is enough to make Italian ancestors turn in their tomato patches. Keep the pineapple in the fruit salad where it belongs!\\\"\\n\\nAt the end of the day, whether pineapple belongs on pizza is a matter of personal taste. Some say it's a match made in heaven, others say it's a culinary crime. But let's face it, in the world of pizza toppings, where you can find everything from anchovies to zucchini, there's room for a little pineapple pizzazz. So, if your taste buds crave that sweet and tangy twist, go ahead and let your pineapple flag fly atop that glorious cheese-covered dough. Bon app\\u00e9tit, or as they say in pineapple paradise, \\\"Aloha-ppetite!\\\" \\ud83c\\udf4d\\ud83c\\udf55\"]\n",
      "\n",
      "=== Response of gpt-4-0613 ===\n",
      "[\"As an AI, I don't have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It's a topic with valid arguments on both sides, and it's up to each individual or organization to decide what they believe is right.\",\"As an AI, I don't eat, so I don't have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness of the fruit has no place on a pizza. It's a pizza puzzle that might never be solved. So, whether pineapple belongs on a pizza or not, it truly depends on your taste buds!\"]\n",
      "--------------------------------------------------\n",
      ">>> gpt-4-1106-preview is the winner!!\n"
     ]
    }
   ],
   "source": [
    "demo_chat = train.iloc[0]\n",
    "for col in [\"prompt\", \"response_a\", \"response_b\"]:\n",
    "    assert isinstance(demo_chat[col], str)\n",
    "    \n",
    "print(f\"=== Prompt ===\")\n",
    "print(demo_chat[\"prompt\"])\n",
    "print()\n",
    "print(f\"=== Response of {demo_chat['model_a']} ===\") \n",
    "print(demo_chat[\"response_a\"])\n",
    "print()\n",
    "print(f\"=== Response of {demo_chat['model_b']} ===\") \n",
    "print(demo_chat[\"response_b\"])\n",
    "print(\"-\" * 50)\n",
    "if demo_chat[\"winner_model_a\"]:\n",
    "    print(f\">>> {demo_chat['model_a']} is the winner!!\")\n",
    "elif demo_chat[\"winner_model_b\"]:\n",
    "    print(f\">>> {demo_chat['model_b']} is the winner!!\")\n",
    "else:\n",
    "    print(f\">>> Winner tie!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate data exploration and pair comparison between two models, we implement a simple chat renderer which can be used to display one chat with Chatbot Arena-like UI. Following shows the same chat as above. Now, we can compare and analyze responses from two models side by side, and the winning status will be shown at the bottom!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatRenderer:\n",
    "    \n",
    "    CSS: str = \"\"\"\n",
    "        <style>\n",
    "            lm-chat-body {\n",
    "                display: flex;\n",
    "                justify-content: center;\n",
    "                align-items: flex-start;\n",
    "                margin: 0;\n",
    "                padding: 20px;\n",
    "                font-family: Arial, sans-serif;\n",
    "                background-color: #f5f5f5;\n",
    "            }\n",
    "            .lm-chat-container {\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "                width: 100%;\n",
    "                max-width: 1200px;\n",
    "                border: 1px solid #ddd;\n",
    "                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "                background-color: #fff;\n",
    "            }\n",
    "            .lm-chat-panels {\n",
    "                display: flex;\n",
    "                width: 100%;\n",
    "            }\n",
    "            .lm-chat-panel {\n",
    "                width: 50%;\n",
    "                padding: 20px;\n",
    "                box-sizing: border-box;\n",
    "                border-right: 1px solid #ddd;\n",
    "                position: relative;\n",
    "            }\n",
    "            .lm-chat-panel:last-child {\n",
    "                border-right: none;\n",
    "            }\n",
    "            .lm-chat-model-header {\n",
    "                font-weight: bold;\n",
    "                margin-bottom: 10px;\n",
    "                font-size: 14px;\n",
    "                position: absolute;\n",
    "                top: 10px;\n",
    "                left: 20px;\n",
    "                background-color: white;\n",
    "                padding: 0 5px;\n",
    "            }\n",
    "            .lm-chat-prompt {\n",
    "                background-color: #fdf5e6;\n",
    "                padding: 10px;\n",
    "                margin-top: 30px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 5px;\n",
    "                border: 1px solid #ddd;\n",
    "            }\n",
    "            .lm-chat-response {\n",
    "                background-color: #fff;\n",
    "                padding: 10px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 5px;\n",
    "                border: 1px solid #ddd;\n",
    "            }\n",
    "            .lm-chat-winner {\n",
    "                text-align: center;\n",
    "                padding: 10px;\n",
    "                border-top: 1px solid #ddd;\n",
    "                background-color: #f5f5f5;\n",
    "                font-size: 16px;\n",
    "            }\n",
    "        </style>\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chat: pd.Series) -> None:\n",
    "        self.model_a, self.model_b = chat[\"model_a\"], chat[\"model_b\"]\n",
    "        self.prompt = chat[\"prompt\"] if isinstance(chat[\"prompt\"], list) else eval(chat[\"prompt\"])\n",
    "        self.res_a = chat[\"response_a\"] if isinstance(chat[\"response_a\"], list) else eval(chat[\"response_a\"])\n",
    "        self.res_b = chat[\"response_b\"] if isinstance(chat[\"response_b\"], list) else eval(chat[\"response_b\"])\n",
    "        self.targets = chat[TARGETS]\n",
    "    \n",
    "    def display(self, suppress_output: bool = False) -> None:\n",
    "        if suppress_output:\n",
    "            with io.capture_output() as captured:\n",
    "                display(self._gen_html())\n",
    "        else:\n",
    "            display(self._gen_html())\n",
    "    \n",
    "    def _gen_html(self) -> HTML:\n",
    "        html_content = f\"\"\"\n",
    "            <script type=\"module\" src=\"https://md-block.verou.me/md-block.js\"></script>\n",
    "            <html>\n",
    "                <lm-chat-body>\n",
    "                    <div class=\"lm-chat-container\">\n",
    "                        <div class=\"lm-chat-panels\">\n",
    "                            {self._gen_panel(\"a\")}\n",
    "                            {self._gen_panel(\"b\")}\n",
    "                        </div>\n",
    "                        <div class=\"lm-chat-winner\">\n",
    "                            {self._get_winner()}\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </lm-chat-body>\n",
    "            </html>\n",
    "        \"\"\".encode(\"utf-16\", \"surrogatepass\").decode(\"utf-16\")\n",
    "        \n",
    "        html = HTML(self.CSS + html_content)\n",
    "        \n",
    "        return html\n",
    "            \n",
    "    def _gen_panel(self, model: str) -> str:\n",
    "        res = self.res_a if model == \"a\" else self.res_b\n",
    "        model_name = self.model_a if model == \"a\" else self.model_b\n",
    "        panel = \"\"\n",
    "        for p, r in zip(self.prompt, res):\n",
    "            panel += f\"\"\"\n",
    "                <div class=\"lm-chat-prompt\">\n",
    "                    <md-block>{p}</md-block>\n",
    "                </div>\n",
    "                <div class=\"lm-chat-response\">\n",
    "                    <md-block>{r}</md-block>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        panel = f\"\"\"\n",
    "            <div class=\"lm-chat-panel\">\n",
    "                <div class=\"lm-chat-model-header\">Model {model.upper()} - {model_name}</div>\n",
    "                {panel}\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "        return panel\n",
    "    \n",
    "    def _get_winner(self) -> str:\n",
    "        if self.targets[\"winner_model_a\"] == 1:\n",
    "            winner = f\"Model A - <strong>{self.model_a}</strong> Wins!\"\n",
    "        elif self.targets[\"winner_model_b\"] == 1:\n",
    "            winner = f\"Model B - <strong>{self.model_b}</strong> Wins!\"\n",
    "        else:\n",
    "            winner = \"Tie!\"\n",
    "        \n",
    "        return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            lm-chat-body {\n",
       "                display: flex;\n",
       "                justify-content: center;\n",
       "                align-items: flex-start;\n",
       "                margin: 0;\n",
       "                padding: 20px;\n",
       "                font-family: Arial, sans-serif;\n",
       "                background-color: #f5f5f5;\n",
       "            }\n",
       "            .lm-chat-container {\n",
       "                display: flex;\n",
       "                flex-direction: column;\n",
       "                width: 100%;\n",
       "                max-width: 1200px;\n",
       "                border: 1px solid #ddd;\n",
       "                box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
       "                background-color: #fff;\n",
       "            }\n",
       "            .lm-chat-panels {\n",
       "                display: flex;\n",
       "                width: 100%;\n",
       "            }\n",
       "            .lm-chat-panel {\n",
       "                width: 50%;\n",
       "                padding: 20px;\n",
       "                box-sizing: border-box;\n",
       "                border-right: 1px solid #ddd;\n",
       "                position: relative;\n",
       "            }\n",
       "            .lm-chat-panel:last-child {\n",
       "                border-right: none;\n",
       "            }\n",
       "            .lm-chat-model-header {\n",
       "                font-weight: bold;\n",
       "                margin-bottom: 10px;\n",
       "                font-size: 14px;\n",
       "                position: absolute;\n",
       "                top: 10px;\n",
       "                left: 20px;\n",
       "                background-color: white;\n",
       "                padding: 0 5px;\n",
       "            }\n",
       "            .lm-chat-prompt {\n",
       "                background-color: #fdf5e6;\n",
       "                padding: 10px;\n",
       "                margin-top: 30px;\n",
       "                margin-bottom: 20px;\n",
       "                border-radius: 5px;\n",
       "                border: 1px solid #ddd;\n",
       "            }\n",
       "            .lm-chat-response {\n",
       "                background-color: #fff;\n",
       "                padding: 10px;\n",
       "                margin-bottom: 20px;\n",
       "                border-radius: 5px;\n",
       "                border: 1px solid #ddd;\n",
       "            }\n",
       "            .lm-chat-winner {\n",
       "                text-align: center;\n",
       "                padding: 10px;\n",
       "                border-top: 1px solid #ddd;\n",
       "                background-color: #f5f5f5;\n",
       "                font-size: 16px;\n",
       "            }\n",
       "        </style>\n",
       "    \n",
       "            <script type=\"module\" src=\"https://md-block.verou.me/md-block.js\"></script>\n",
       "            <html>\n",
       "                <lm-chat-body>\n",
       "                    <div class=\"lm-chat-container\">\n",
       "                        <div class=\"lm-chat-panels\">\n",
       "                            \n",
       "            <div class=\"lm-chat-panel\">\n",
       "                <div class=\"lm-chat-model-header\">Model A - gpt-4-1106-preview</div>\n",
       "                \n",
       "                <div class=\"lm-chat-prompt\">\n",
       "                    <md-block>Is it morally right to try to have a certain percentage of females on managerial positions?</md-block>\n",
       "                </div>\n",
       "                <div class=\"lm-chat-response\">\n",
       "                    <md-block>The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\n",
       "\n",
       "Here are some arguments in favor of and against such policies:\n",
       "\n",
       "**Arguments in favor:**\n",
       "\n",
       "1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\n",
       "\n",
       "2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\n",
       "\n",
       "3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\n",
       "\n",
       "4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\n",
       "\n",
       "**Arguments against:**\n",
       "\n",
       "1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\n",
       "\n",
       "2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\n",
       "\n",
       "3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \"tokens,\" undermining their legitimacy and potentially leading to resentment among colleagues.\n",
       "\n",
       "4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\n",
       "\n",
       "Ultimately, the morality of aiming for a certain percentage of females in managerial positions depends on one's ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \"color-blind\" or \"gender-blind\" approach to hiring and promotions.\n",
       "\n",
       "Organizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions.</md-block>\n",
       "                </div>\n",
       "            \n",
       "                <div class=\"lm-chat-prompt\">\n",
       "                    <md-block>OK, does pineapple belong on a pizza? Relax and give me fun answer.</md-block>\n",
       "                </div>\n",
       "                <div class=\"lm-chat-response\">\n",
       "                    <md-block>Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battle and pizza purists protest.\n",
       "\n",
       "Let's slice into the debate with a zest of fun:\n",
       "\n",
       "**Team Pineapple:** \"Absolutely, yes! Pineapple on pizza is like a beach party in your mouth. The sweet juiciness of pineapple chunks frolicking with savory ham or bacon creates a flavor wave that surfs across the cheesy ocean of deliciousness. It's the Mardi Gras of pizzas, where sweet meets savory in a jubilant jamboree!\"\n",
       "\n",
       "**Team No-Pineapple:** \"No way, not in a million pizzas! Pineapple is a fruit that should be sunbathing on a tropical fruit platter, not freeloading on a sacred slice of pizza. The thought of warm, melty cheese conspiring with pineapple's sugary mischief is enough to make Italian ancestors turn in their tomato patches. Keep the pineapple in the fruit salad where it belongs!\"\n",
       "\n",
       "At the end of the day, whether pineapple belongs on pizza is a matter of personal taste. Some say it's a match made in heaven, others say it's a culinary crime. But let's face it, in the world of pizza toppings, where you can find everything from anchovies to zucchini, there's room for a little pineapple pizzazz. So, if your taste buds crave that sweet and tangy twist, go ahead and let your pineapple flag fly atop that glorious cheese-covered dough. Bon app√©tit, or as they say in pineapple paradise, \"Aloha-ppetite!\" üççüçï</md-block>\n",
       "                </div>\n",
       "            \n",
       "            </div>\n",
       "        \n",
       "                            \n",
       "            <div class=\"lm-chat-panel\">\n",
       "                <div class=\"lm-chat-model-header\">Model B - gpt-4-0613</div>\n",
       "                \n",
       "                <div class=\"lm-chat-prompt\">\n",
       "                    <md-block>Is it morally right to try to have a certain percentage of females on managerial positions?</md-block>\n",
       "                </div>\n",
       "                <div class=\"lm-chat-response\">\n",
       "                    <md-block>As an AI, I don't have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It's a topic with valid arguments on both sides, and it's up to each individual or organization to decide what they believe is right.</md-block>\n",
       "                </div>\n",
       "            \n",
       "                <div class=\"lm-chat-prompt\">\n",
       "                    <md-block>OK, does pineapple belong on a pizza? Relax and give me fun answer.</md-block>\n",
       "                </div>\n",
       "                <div class=\"lm-chat-response\">\n",
       "                    <md-block>As an AI, I don't eat, so I don't have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness of the fruit has no place on a pizza. It's a pizza puzzle that might never be solved. So, whether pineapple belongs on a pizza or not, it truly depends on your taste buds!</md-block>\n",
       "                </div>\n",
       "            \n",
       "            </div>\n",
       "        \n",
       "                        </div>\n",
       "                        <div class=\"lm-chat-winner\">\n",
       "                            Model A - <strong>gpt-4-1106-preview</strong> Wins!\n",
       "                        </div>\n",
       "                    </div>\n",
       "                </lm-chat-body>\n",
       "            </html>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_renderer = ChatRenderer(train.iloc[0])\n",
    "chat_renderer.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"feat_and_tgt\"></a>\n",
    "## 2. Feature and Target Exploration\n",
    "We've already known what information is included in one sample. Next, we're going to take a deep dive into each feature!\n",
    "\n",
    "<a id=\"model_pair\"></a>\n",
    "### *Model Pair - (`model_a`, `model_b`)*\n",
    "\n",
    "Let's focus on LLM compositions for both `model_a` and `model_b`. Observations are summarized as follows,\n",
    "1. All model identities in the training set are provided (*i.e.,* no missing or empty entry exists).\n",
    "2. Model compositions of `model_a` and `model_b` are slightly different.\n",
    "    * Both have the same 64 unique models.\n",
    "    * Both have the same top-4 most frequent model identities, `gpt-4-1106-preview --> gpt-3.5-turbo-0613 --> gpt-4-0613 --> claude-2.1`.\n",
    "    * The 5-th most frequent model is `gpt-4-0314` for `model_a` and `claude-instant-1` for `model_b`.\n",
    "3. Most models account for less than 2%, which are combined into a single `Others` class for better visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There exist 0 unknown model_a and 0 unknown model_b identities.\n"
     ]
    }
   ],
   "source": [
    "unknown_model_a = train[\"model_a\"].isna().sum() + (train[\"model_a\"] == \"\").sum()\n",
    "unknown_model_b = train[\"model_b\"].isna().sum() + (train[\"model_b\"] == \"\").sum()\n",
    "print(f\"There exist {unknown_model_a} unknown model_a and {unknown_model_b} unknown model_b identities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_plot_pie\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     data: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m      3\u001b[0m     labels: List[\u001b[38;5;28mstr\u001b[39m], \n\u001b[1;32m      4\u001b[0m     title: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      5\u001b[0m     combine_minority: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     thres: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m, \n\u001b[1;32m      7\u001b[0m     max_labels: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      8\u001b[0m     ax: Optional[Axes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     tot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(data)\n\u001b[1;32m     11\u001b[0m     max_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def _plot_pie(\n",
    "    data: np.ndarray,\n",
    "    labels: List[str], \n",
    "    title: str,\n",
    "    combine_minority: bool = False,\n",
    "    thres: float = 0.01, \n",
    "    max_labels: int = 10,\n",
    "    ax: Optional[Axes] = None,\n",
    ") -> None:\n",
    "    tot = sum(data)\n",
    "    max_idx = np.argmax(data)\n",
    "    if combine_minority:\n",
    "        major_data = [(d, l) for d, l in zip(data, labels) if d / tot >= thres]\n",
    "        minor_data = [(d, l) for d, l in zip(data, labels) if d / tot < thres]\n",
    "        \n",
    "        if len(minor_data) != 0:\n",
    "            # Add one \"Others\" class to store minority classes\n",
    "            minor_tot = sum(d for d, l in minor_data)\n",
    "            major_data.append((minor_tot, \"Others\"))\n",
    "        \n",
    "        data, labels = map(list, zip(*major_data))\n",
    "        max_idx = np.argmax(data[:-1])\n",
    "        \n",
    "    # Sort data/labels\n",
    "    # ...\n",
    "    explode = [0.1 if i == max_idx else 0 for i in range(len(data))]    \n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    if len(data) > max_labels:\n",
    "        visible_labels = labels[:max_labels] + [\"\"] * (len(data) - max_labels)\n",
    "    else:\n",
    "        visible_labels = labels\n",
    "    patches, texts = ax.pie(data, startangle=140, colors=colors, explode=explode, labels=visible_labels)\n",
    "    ax.legend(patches, labels, title=\"Model Identity\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    if ax is None:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boosting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
